{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we now build the index which is our knowledge base.\n",
    "These are papers of a specific topic that are already in the database or repository(arxiv database).\n",
    "NB:Indexes are built to handle data in a large dataset.\n",
    "'''\n",
    "\n",
    "from tools import fetch_arxiv_papers\n",
    "\n",
    "papers = fetch_arxiv_papers('Language Models', 10) # Fetch 10 papers on Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an index when creating an AI agent serves as a foundation for efficient data storage, retrieval, and processing. The index enables the agent to access relevant information quickly and accurately, which is crucial for tasks such as answering questions, making decisions, or generating content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Distilling Multi-modal Large Language Models for Autonomous Driving',\n",
       " 'Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues',\n",
       " 'OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking',\n",
       " 'Enhancing Lexicon-Based Text Embeddings with Large Language Models',\n",
       " 'Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models',\n",
       " 'Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps',\n",
       " 'A Simple Aerial Detection Baseline of Multimodal Language Models',\n",
       " 'CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education',\n",
       " 'Domain Adaptation of Foundation LLMs for e-Commerce',\n",
       " 'Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper['title'] for paper in papers] # Print the titles of the fetched papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create documents from the fetched papers\n",
    "#the documents are generic interface for the data that we want to index\n",
    "#the documemt is connected to data sources\n",
    "\n",
    "from llama_index.core  import Document\n",
    "\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        content = (\n",
    "            f'Title: {paper[\"title\"]}\\n'\n",
    "            f'Authors: {\", \".join(paper[\"authors\"])}\\n'\n",
    "            f'summary: {paper[\"summary\"]}\\n'\n",
    "            f'Published: {paper[\"published\"]}\\n'\n",
    "            f'journal_ref: {paper[\"journal_ref\"]}\\n'\n",
    "            f'ODOI: {paper[\"doi\"]}\\n'\n",
    "            f'Primary Category: {paper[\"primary_category\"]}\\n'\n",
    "            f'categories: {\", \".join(paper[\"categories\"])}\\n'\n",
    "            f'PDF url: {paper[\"pdf_url\"]}\\n'\n",
    "            f'arxiv url: {paper[\"arxiv_url\"]}\\n'\n",
    "        )\n",
    "        documents.append(Document(text = content))\n",
    "    return documents\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the create_documnets_from_papers function\n",
    "documents = create_documents_from_papers( papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='f085c0bc-6d17-413e-93d6-8719e925ef25', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Distilling Multi-modal Large Language Models for Autonomous Driving\\nAuthors: Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli\\nsummary: Autonomous driving demands safe motion planning, especially in critical\\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\\nlarge language models (LLMs) as planners to improve generalizability to rare\\nevents. However, using LLMs at test time introduces high computational costs.\\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\\nmaintains the efficiency of an LLM-free (or vision-based) planner while\\nleveraging the world knowledge of an LLM. DiMA distills the information from a\\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\\ncommon to both networks produces structured representations that are\\nsemantically grounded as well as aligned to the final planning objective.\\nNotably, the LLM is optional at inference, enabling robust planning without\\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\\nthe L2 trajectory error and an 80% reduction in the collision rate of the\\nvision-based planner, as well as a 44% trajectory error reduction in longtail\\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\\nplanning benchmark.\\nPublished: 2025-01-16 18:59:53+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CV\\ncategories: cs.CV, cs.RO\\nPDF url: http://arxiv.org/pdf/2501.09757v1\\narxiv url: http://arxiv.org/abs/2501.09757v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dc403918-a86b-4bce-847a-67f9e0912362', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues\\nAuthors: Youngjoon Jang, Haran Raajesh, Liliane Momeni, GÃ¼l Varol, Andrew Zisserman\\nsummary: Our objective is to translate continuous sign language into spoken language\\ntext. Inspired by the way human interpreters rely on context for accurate\\ntranslation, we incorporate additional contextual cues together with the\\nsigning video, into a new translation framework. Specifically, besides visual\\nsign recognition features that encode the input video, we integrate\\ncomplementary textual information from (i) captions describing the background\\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\\ntranscribing the signing. These are automatically extracted and inputted along\\nwith the visual features to a pre-trained large language model (LLM), which we\\nfine-tune to generate spoken language translations in text form. Through\\nextensive ablation studies, we show the positive contribution of each input cue\\nto the translation performance. We train and evaluate our approach on BOBSL --\\nthe largest British Sign Language dataset currently available. We show that our\\ncontextual approach significantly enhances the quality of the translations\\ncompared to previously reported results on BOBSL, and also to state-of-the-art\\nmethods that we implement as baselines. Furthermore, we demonstrate the\\ngenerality of our approach by applying it also to How2Sign, an American Sign\\nLanguage dataset, and achieve competitive results.\\nPublished: 2025-01-16 18:59:03+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CV\\ncategories: cs.CV\\nPDF url: http://arxiv.org/pdf/2501.09754v1\\narxiv url: http://arxiv.org/abs/2501.09754v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='75829316-bc5f-4d9b-b823-ec9248421be4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking\\nAuthors: Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen\\nsummary: Machine writing with large language models often relies on\\nretrieval-augmented generation. However, these approaches remain confined\\nwithin the boundaries of the model's predefined scope, limiting the generation\\nof content with rich information. Specifically, vanilla-retrieved information\\ntends to lack depth, utility, and suffers from redundancy, which negatively\\nimpacts the quality of generated articles, leading to shallow, repetitive, and\\nunoriginal outputs. To address these issues, we propose OmniThink, a machine\\nwriting framework that emulates the human-like process of iterative expansion\\nand reflection. The core idea behind OmniThink is to simulate the cognitive\\nbehavior of learners as they progressively deepen their knowledge of the\\ntopics. Experimental results demonstrate that OmniThink improves the knowledge\\ndensity of generated articles without compromising metrics such as coherence\\nand depth. Human evaluations and expert feedback further highlight the\\npotential of OmniThink to address real-world challenges in the generation of\\nlong-form articles.\\nPublished: 2025-01-16 18:58:06+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CL\\ncategories: cs.CL\\nPDF url: http://arxiv.org/pdf/2501.09751v1\\narxiv url: http://arxiv.org/abs/2501.09751v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d4bb7f3a-ef66-4cfc-b2c4-65cb61613aee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Enhancing Lexicon-Based Text Embeddings with Large Language Models\\nAuthors: Yibin Lei, Tao Shen, Yu Cao, Andrew Yates\\nsummary: Recent large language models (LLMs) have demonstrated exceptional performance\\non general-purpose text embedding tasks. While dense embeddings have dominated\\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\\nthe inherent tokenization redundancy issue and unidirectional attention\\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\\nthrough token embedding clustering, and investigates bidirectional attention\\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\\nby assigning each dimension to a specific token cluster, where semantically\\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\\ndelivering compact feature representations that match the sizes of dense\\ncounterparts. Notably, combining LENSE with dense embeddings achieves\\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).\\nPublished: 2025-01-16 18:57:20+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CL\\ncategories: cs.CL, cs.IR\\nPDF url: http://arxiv.org/pdf/2501.09749v1\\narxiv url: http://arxiv.org/abs/2501.09749v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a6a64a6-2b36-4005-8d81-2a94d78dd34b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models\\nAuthors: Bihui Jin, Jiayue Wang, Pengyu Nie\\nsummary: Machine learning developers frequently use interactive computational\\nnotebooks, such as Jupyter notebooks, to host code for data processing and\\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\\nlearning pipelines and interactively observing outputs, however, maintaining\\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\\ndue to the length and complexity of the notebooks. Moreover, there is no\\nexisting benchmark related to developer edits on Jupyter notebooks. To address\\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\\nperform the first study of the using LLMs to predict code edits in Jupyter\\nnotebooks. Our dataset captures granular details of cell-level and line-level\\nmodifications, offering a foundation for understanding real-world maintenance\\npatterns in machine learning workflows. We observed that the edits on Jupyter\\nnotebooks are highly localized, with changes averaging only 166 lines of code\\nin repositories. While larger models outperform smaller counterparts in code\\nediting, all models have low accuracy on our dataset even after finetuning,\\ndemonstrating the complexity of real-world machine learning maintenance tasks.\\nOur findings emphasize the critical role of contextual information in improving\\nmodel performance and point toward promising avenues for advancing large\\nlanguage models' capabilities in engineering machine learning code.\\nPublished: 2025-01-16 18:55:38+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.SE\\ncategories: cs.SE, cs.CL, cs.LG\\nPDF url: http://arxiv.org/pdf/2501.09745v1\\narxiv url: http://arxiv.org/abs/2501.09745v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2f68837a-ef6c-4d56-bdc3-7d8b921a6453', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps\\nAuthors: Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie\\nsummary: Generative models have made significant impacts across various domains,\\nlargely due to their ability to scale during training by increasing data,\\ncomputational resources, and model size, a phenomenon characterized by the\\nscaling laws. Recent research has begun to explore inference-time scaling\\nbehavior in Large Language Models (LLMs), revealing how performance can further\\nimprove with additional computation during inference. Unlike LLMs, diffusion\\nmodels inherently possess the flexibility to adjust inference-time computation\\nvia the number of denoising steps, although the performance gains typically\\nflatten after a few dozen. In this work, we explore the inference-time scaling\\nbehavior of diffusion models beyond increasing denoising steps and investigate\\nhow the generation performance can further improve with increased computation.\\nSpecifically, we consider a search problem aimed at identifying better noises\\nfor the diffusion sampling process. We structure the design space along two\\naxes: the verifiers used to provide feedback, and the algorithms used to find\\nbetter noise candidates. Through extensive experiments on class-conditioned and\\ntext-conditioned image generation benchmarks, our findings reveal that\\nincreasing inference-time compute leads to substantial improvements in the\\nquality of samples generated by diffusion models, and with the complicated\\nnature of images, combinations of the components in the framework can be\\nspecifically chosen to conform with different application scenario.\\nPublished: 2025-01-16 18:30:37+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CV\\ncategories: cs.CV\\nPDF url: http://arxiv.org/pdf/2501.09732v1\\narxiv url: http://arxiv.org/abs/2501.09732v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='06ca275a-a8e9-483c-9f60-f9c03a4d4d11', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: A Simple Aerial Detection Baseline of Multimodal Language Models\\nAuthors: Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang\\nsummary: The multimodal language models (MLMs) based on generative pre-trained\\nTransformer are considered powerful candidates for unifying various domains and\\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\\nperformance in multiple tasks, such as visual question answering and visual\\ngrounding. In addition to visual grounding that detects specific objects\\ncorresponded to given instruction, aerial detection, which detects all objects\\nof multiple categories, is also a valuable and challenging task for RS\\nfoundation models. However, aerial detection has not been explored by existing\\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\\nsignificantly from the detection outputs. In this paper, we present a simple\\nbaseline for applying MLMs to aerial detection for the first time, named\\nLMMRotate. Specifically, we first introduce a normalization method to transform\\ndetection outputs into textual outputs to be compatible with the MLM framework.\\nThen, we propose a evaluation method, which ensures a fair comparison between\\nMLMs and conventional object detection models. We construct the baseline by\\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\\nperformance comparable to conventional detector. We hope that this baseline\\nwill serve as a reference for future MLM development, enabling more\\ncomprehensive capabilities for understanding RS images. Code is available at\\nhttps://github.com/Li-Qingyun/mllm-mmrotate.\\nPublished: 2025-01-16 18:09:22+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CV\\ncategories: cs.CV, cs.AI\\nPDF url: http://arxiv.org/pdf/2501.09720v1\\narxiv url: http://arxiv.org/abs/2501.09720v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa6651f4-b3d8-4245-b1a8-e72f408d4c71', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education\\nAuthors: Tianyu Wang, Nianjun Zhou, Zhixiong Chen\\nsummary: Many non-traditional students in cybersecurity programs often lack access to\\nadvice from peers, family members and professors, which can hinder their\\neducational experiences. Additionally, these students may not fully benefit\\nfrom various LLM-powered AI assistants due to issues like content relevance,\\nlocality of advice, minimum expertise, and timing. This paper addresses these\\nchallenges by introducing an application designed to provide comprehensive\\nsupport by answering questions related to knowledge, skills, and career\\npreparation advice tailored to the needs of these students. We developed a\\nlearning tool platform, CyberMentor, to address the diverse needs and pain\\npoints of students majoring in cybersecurity. Powered by agentic workflow and\\nGenerative Large Language Models (LLMs), the platform leverages\\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\\ninformation retrieval to achieve accessibility and personalization. We\\ndemonstrated its value in addressing knowledge requirements for cybersecurity\\neducation and for career marketability, in tackling skill requirements for\\nanalytical and programming assignments, and in delivering real time on demand\\nlearning support. Using three use scenarios, we showcased CyberMentor in\\nfacilitating knowledge acquisition and career preparation and providing\\nseamless skill-based guidance and support. We also employed the LangChain\\nprompt-based evaluation methodology to evaluate the platform's impact,\\nconfirming its strong performance in helpfulness, correctness, and\\ncompleteness. These results underscore the system's ability to support students\\nin developing practical cybersecurity skills while improving equity and\\nsustainability within higher education. Furthermore, CyberMentor's open-source\\ndesign allows for adaptation across other disciplines, fostering educational\\ninnovation and broadening its potential impact.\\nPublished: 2025-01-16 18:00:06+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CY\\ncategories: cs.CY, cs.AI, K.3.2; I.2.1\\nPDF url: http://arxiv.org/pdf/2501.09709v1\\narxiv url: http://arxiv.org/abs/2501.09709v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3e39c10b-8125-4587-bc38-ad67ebed9a45', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Domain Adaptation of Foundation LLMs for e-Commerce\\nAuthors: Christian Herold, Michael Kozielski, Tala Bazazo, Pavel Petrushkov, Hadi Hashemi, Patrycja Cieplicka, Dominika Basaj, Shahram Khadivi\\nsummary: We present the e-Llama models: 8 billion and 70 billion parameter large\\nlanguage models that are adapted towards the e-commerce domain. These models\\nare meant as foundation models with deep knowledge about e-commerce, that form\\na base for instruction- and fine-tuning. The e-Llama models are obtained by\\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\\ndomain-specific data.\\n  We discuss our approach and motivate our choice of hyperparameters with a\\nseries of ablation studies. To quantify how well the models have been adapted\\nto the e-commerce domain, we define and implement a set of multilingual,\\ne-commerce specific evaluation tasks.\\n  We show that, when carefully choosing the training setup, the Llama 3.1\\nmodels can be adapted towards the new domain without sacrificing significant\\nperformance on general domain tasks. We also explore the possibility of merging\\nthe adapted model and the base model for a better control of the performance\\ntrade-off between domains.\\nPublished: 2025-01-16 17:58:32+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CL\\ncategories: cs.CL\\nPDF url: http://arxiv.org/pdf/2501.09706v1\\narxiv url: http://arxiv.org/abs/2501.09706v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='315fda6f-62ab-4a2b-864a-807451e3f00d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key\\nAuthors: Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li\\nsummary: Hallucination remains a major challenge for Large Vision-Language Models\\n(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention\\nas a simple solution to hallucination issues. It directly learns from\\nconstructed preference pairs that reflect the severity of hallucinations in\\nresponses to the same prompt and image. Nonetheless, different data\\nconstruction methods in existing works bring notable performance variations. We\\nidentify a crucial factor here: outcomes are largely contingent on whether the\\nconstructed data aligns on-policy w.r.t the initial (reference) policy of DPO.\\nTheoretical analysis suggests that learning from off-policy data is impeded by\\nthe presence of KL-divergence between the updated policy and the reference\\npolicy. From the perspective of dataset distribution, we systematically\\nsummarize the inherent flaws in existing algorithms that employ DPO to address\\nhallucination issues. To alleviate the problems, we propose On-Policy Alignment\\n(OPA)-DPO framework, which uniquely leverages expert feedback to correct\\nhallucinated responses and aligns both the original and expert-revised\\nresponses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO\\nachieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:\\n13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared\\nto the previous SOTA algorithm trained with 16k samples.\\nPublished: 2025-01-16 17:48:03+00:00\\njournal_ref: None\\nODOI: None\\nPrimary Category: cs.CV\\ncategories: cs.CV\\nPDF url: http://arxiv.org/pdf/2501.09695v1\\narxiv url: http://arxiv.org/abs/2501.09695v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#builds a string that will be parse through the embed model\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#% ensure we are installing in the correct environment\\n%pip install constants\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#% ensure we are installing in the correct environment\n",
    "%pip install constants\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#print the content of constant to determine what it contains\\n\\nimport constants\\nprint(dir(constants))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#print the content of constant to determine what it contains\n",
    "\n",
    "import constants\n",
    "print(dir(constants))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthis is actually calling the openai model to embed the documents.that is to change the documents into vectors\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#this where we build the index\n",
    "\n",
    "from llama_index.core import Settings,VectorStoreIndex\n",
    "from constant import embed_model\n",
    "\n",
    "Settings.chunk_size = 1024   #the text will be processed in chunks of 1024 characters(if the text in more than 1024,it will be processed in 2 chunks)\n",
    "Settings.chunk_overlap = 50  #each chunk will overlap with the previous one by 50 characters\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model = embed_model)\n",
    "'''\n",
    "In creating the index,we pass the documents through and embed_model\n",
    "This code snippet is actually calling the openai model to embed the documents.that is to change the documents into vectors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the created index in a folder called index(index/ is a local storage)\n",
    "index.storage_context.persist('index/')  \n",
    "\n",
    "'''\n",
    "Add the index to gitignore because it changes based on what we search for and the documents we index.\n",
    "go to .gitignore and add index/ (this is a local index)\n",
    "\n",
    "we can also use cloud-based storage index like pinecone or weaviate\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
