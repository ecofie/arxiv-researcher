{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "We are now going to build the Agent\n",
    "first,we load the index using the storage context and the load_index_from_storage function\n",
    "'''\n",
    "\n",
    "from llama_index.core import StorageContext,load_index_from_storage\n",
    "from constant import embed_model\n",
    "\n",
    "#load the index\n",
    "storage_context = StorageContext.from_defaults(persist_dir='index/')\n",
    "index = load_index_from_storage(storage_context,embed_model=embed_model) #we parse the storage context through the load_index_from_storage function and the embed_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the RAG tool\n",
    "This is the tool the agent will use to fetch information from the existing database\n",
    "'''\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from constant import llm_model\n",
    "\n",
    "\n",
    "#load the llm model engine\n",
    "query_engine= index.as_query_engine(llm_model=llm_model, similarity_top_k=4) #we find 5 max similarity vectors\n",
    "\n",
    "#the tool the engine will use to fetch information\n",
    "rag_tool=QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name = \"research_paper_query_engine_tool\",\n",
    "    description = \"A RAG engine with recent Research paper\",\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function to display the prompt\n",
    "from IPython.display import Markdown,display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for key, prompt in prompts_dict.items():\n",
    "        display(Markdown(f\"**Prompt key**: {key}\"))\n",
    "        print(prompt.get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:text_qa_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:refine_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the other tools the agent will use\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from tools import download_pdf, fetch_arxiv_papers\n",
    "\n",
    "download_pdf_tool = FunctionTool.from_defaults(\n",
    "    download_pdf, \n",
    "    name=\"download_pdf_file_tool\", \n",
    "    description=\"pyhton function that downloads a pdfs file by link\",\n",
    "    )\n",
    "\n",
    "fetch_arxiv_tool = FunctionTool.from_defaults(\n",
    "    fetch_arxiv_papers, \n",
    "    name=\"fetch_from_arxiv\", \n",
    "    description=\"download the {max_result} recent papers regarding the topic {title} from arvix\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the Agent(We use the ReAct beacuse the agent will be able to react to the user input)\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    [download_pdf_tool,rag_tool,fetch_arxiv_tool],llm=llm_model,verbose=True\n",
    ")   #parse the list of tools the agent will use and the llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to be able to chat with the agent,we need a query template\n",
    "'''\n",
    "query_template = ''' I am interested in {topic}.\n",
    "Find papers in your knowledge base that are related to topic.\n",
    "Use the following template to query research_paper_query_engine_tool tool:Provide title, summary, authors and links to download to pappers related to {topic}.\n",
    "if there are not,could you search the recent one from arxiv\n",
    "IMPORTANT :do not download papers unless the user asks for it explicitly.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step eb620a0a-fb8b-4e9c-87af-3a9e735ddd45. Step input:  I am interested in Multi-modal models.\n",
      "Find papers in your knowledge base that are related to topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool:Provide title, summary, authors and links to download to pappers related to Multi-modal models.\n",
      "if there are not,could you search the recent one from arxiv\n",
      "IMPORTANT :do not download papers unless the user asks for it explicitly.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me find research papers related to Multi-modal models.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Multi-modal models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Context information is below.\n",
      "---------------------\n",
      "Title: Distilling Multi-modal Large Language Models for Autonomous Driving\n",
      "Authors: Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli\n",
      "summary: Autonomous driving demands safe motion planning, especially in critical\n",
      "\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\n",
      "large language models (LLMs) as planners to improve generalizability to rare\n",
      "events. However, using LLMs at test time introduces high computational costs.\n",
      "To address this, we propose DiMA, an end-to-end autonomous driving system that\n",
      "maintains the efficiency of an LLM-free (or vision-based) planner while\n",
      "leveraging the world knowledge of an LLM. DiMA distills the information from a\n",
      "multi-modal LLM to a vision-based end-to-end planner through a set of specially\n",
      "designed surrogate tasks. Under a joint training strategy, a scene encoder\n",
      "common to both networks produces structured representations that are\n",
      "semantically grounded as well as aligned to the final planning objective.\n",
      "Notably, the LLM is optional at inference, enabling robust planning without\n",
      "compromising on efficiency. Training with DiMA results in a 37% reduction in\n",
      "the L2 trajectory error and an 80% reduction in the collision rate of the\n",
      "vision-based planner, as well as a 44% trajectory error reduction in longtail\n",
      "scenarios. DiMA also achieves state-of-the-art performance on the nuScenes\n",
      "planning benchmark.\n",
      "Published: 2025-01-16 18:59:53+00:00\n",
      "journal_ref: None\n",
      "ODOI: None\n",
      "Primary Category: cs.CV\n",
      "categories: cs.CV, cs.RO\n",
      "PDF url: http://arxiv.org/pdf/2501.09757v1\n",
      "arxiv url: http://arxiv.org/abs/2501.09757v1\n",
      "\n",
      "Title: A Simple Aerial Detection Baseline of Multimodal Language Models\n",
      "Authors: Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang\n",
      "summary: The multimodal language models (MLMs) based on generative pre-trained\n",
      "Transformer are considered powerful candidates for unifying various domains and\n",
      "tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\n",
      "performance in multiple tasks, such as visual question answering and visual\n",
      "grounding. In addition to visual grounding that detects specific objects\n",
      "corresponded to given instruction, aerial detection, which detects all objects\n",
      "of multiple categories, is also a valuable and challenging task for RS\n",
      "foundation models. However, aerial detection has not been explored by existing\n",
      "RS MLMs because the autoregressive prediction mechanism of MLMs differs\n",
      "significantly from the detection outputs. In this paper, we present a simple\n",
      "baseline for applying MLMs to aerial detection for the first time, named\n",
      "LMMRotate. Specifically, we first introduce a normalization method to transform\n",
      "detection outputs into textual outputs to be compatible with the MLM framework.\n",
      "Then, we propose a evaluation method, which ensures a fair comparison between\n",
      "MLMs and conventional object detection models. We construct the baseline by\n",
      "fine-tuning open-source general-purpose MLMs and achieve impressive detection\n",
      "performance comparable to conventional detector. We hope that this baseline\n",
      "will serve as a reference for future MLM development, enabling more\n",
      "comprehensive capabilities for understanding RS images. Code is available at\n",
      "https://github.com/Li-Qingyun/mllm-mmrotate.\n",
      "Published: 2025-01-16 18:09:22+00:00\n",
      "journal_ref: None\n",
      "ODOI: None\n",
      "Primary Category: cs.CV\n",
      "categories: cs.CV, cs.AI\n",
      "PDF url: http://arxiv.org/pdf/2501.09720v1\n",
      "arxiv url: http://arxiv.org/abs/2501.09720v1\n",
      "\n",
      "Title: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps\n",
      "Authors: Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie\n",
      "summary: Generative models have made significant impacts across various domains,\n",
      "largely due to their ability to scale during training by increasing data,\n",
      "computational resources, and model size, a phenomenon characterized by the\n",
      "scaling laws. Recent research has begun to explore inference-time scaling\n",
      "behavior in Large Language Models (LLMs), revealing how performance can further\n",
      "improve with additional computation during inference. Unlike LLMs, diffusion\n",
      "models inherently possess the flexibility to adjust inference-time computation\n",
      "via the number of denoising steps, although the performance gains typically\n",
      "flatten after a few dozen. In this work, we explore the inference-time scaling\n",
      "behavior of diffusion models beyond increasing denoising steps and investigate\n",
      "how the generation performance can further improve with increased computation.\n",
      "Specifically, we consider a search problem aimed at identifying better noises\n",
      "for the diffusion sampling process. We structure the design space along two\n",
      "axes: the verifiers used to provide feedback, and the algorithms used to find\n",
      "better noise candidates. Through extensive experiments on class-conditioned and\n",
      "text-conditioned image generation benchmarks, our findings reveal that\n",
      "increasing inference-time compute leads to substantial improvements in the\n",
      "quality of samples generated by diffusion models, and with the complicated\n",
      "nature of images, combinations of the components in the framework can be\n",
      "specifically chosen to conform with different application scenario.\n",
      "Published: 2025-01-16 18:30:37+00:00\n",
      "journal_ref: None\n",
      "ODOI: None\n",
      "Primary Category: cs.CV\n",
      "categories: cs.CV\n",
      "PDF url: http://arxiv.org/pdf/2501.09732v1\n",
      "arxiv url: http://arxiv.org/abs/2501.09732v1\n",
      "\n",
      "Title: Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key\n",
      "Authors: Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li\n",
      "summary: Hallucination remains a major challenge for Large Vision-Language Models\n",
      "(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention\n",
      "as a simple solution to hallucination issues. It directly learns from\n",
      "constructed preference pairs that reflect the severity of hallucinations in\n",
      "responses to the same prompt and image. Nonetheless, different data\n",
      "construction methods in existing works bring notable performance variations. We\n",
      "identify a crucial factor here: outcomes are largely contingent on whether the\n",
      "constructed data aligns on-policy w.r.t the initial (reference) policy of DPO.\n",
      "Theoretical analysis suggests that learning from off-policy data is impeded by\n",
      "the presence of KL-divergence between the updated policy and the reference\n",
      "policy. From the perspective of dataset distribution, we systematically\n",
      "summarize the inherent flaws in existing algorithms that employ DPO to address\n",
      "hallucination issues. To alleviate the problems, we propose On-Policy Alignment\n",
      "(OPA)-DPO framework, which uniquely leverages expert feedback to correct\n",
      "hallucinated responses and aligns both the original and expert-revised\n",
      "responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO\n",
      "achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:\n",
      "13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared\n",
      "to the previous SOTA algorithm trained with 16k samples.\n",
      "Published: 2025-01-16 17:48:03+00:00\n",
      "journal_ref: None\n",
      "ODOI: None\n",
      "Primary Category: cs.CV\n",
      "categories: cs.CV\n",
      "PDF url: http://arxiv.org/pdf/2501.09695v1\n",
      "arxiv url: http://arxiv.org/abs/2501.09695v1\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Multi-modal models\n",
      "Answer: \n",
      "\u001b[0m> Running step 6c4dc4c5-d50a-4b49-afea-39e32e41db4e. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Here are some recent papers related to Multi-modal models:\n",
      "\n",
      "1. **Title**: Distilling Multi-modal Large Language Models for Autonomous Driving  \n",
      "   **Authors**: Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli  \n",
      "   **Summary**: This paper proposes DiMA, an end-to-end autonomous driving system that distills information from a multi-modal large language model (LLM) to a vision-based planner, improving efficiency and reducing trajectory errors.  \n",
      "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09757v1) | [arXiv Link](http://arxiv.org/abs/2501.09757v1)\n",
      "\n",
      "2. **Title**: A Simple Aerial Detection Baseline of Multimodal Language Models  \n",
      "   **Authors**: Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang  \n",
      "   **Summary**: This paper presents a baseline for applying multimodal language models (MLMs) to aerial detection in remote sensing, achieving impressive detection performance comparable to conventional models.  \n",
      "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09720v1) | [arXiv Link](http://arxiv.org/abs/2501.09720v1)\n",
      "\n",
      "3. **Title**: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps  \n",
      "   **Authors**: Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie  \n",
      "   **Summary**: This work explores the inference-time scaling behavior of diffusion models, revealing how performance can improve with increased computation during inference.  \n",
      "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09732v1) | [arXiv Link](http://arxiv.org/abs/2501.09732v1)\n",
      "\n",
      "4. **Title**: Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key  \n",
      "   **Authors**: Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li  \n",
      "   **Summary**: This paper addresses hallucination issues in large vision-language models using Direct Preference Optimization (DPO) and proposes an On-Policy Alignment framework to improve response accuracy.  \n",
      "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09695v1) | [arXiv Link](http://arxiv.org/abs/2501.09695v1)\n",
      "\n",
      "These papers provide insights into various aspects of multi-modal models and their applications.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic= \"Multi-modal models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers related to Multi-modal models:\n",
       "\n",
       "1. **Title**: Distilling Multi-modal Large Language Models for Autonomous Driving  \n",
       "   **Authors**: Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli  \n",
       "   **Summary**: This paper proposes DiMA, an end-to-end autonomous driving system that distills information from a multi-modal large language model (LLM) to a vision-based planner, improving efficiency and reducing trajectory errors.  \n",
       "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09757v1) | [arXiv Link](http://arxiv.org/abs/2501.09757v1)\n",
       "\n",
       "2. **Title**: A Simple Aerial Detection Baseline of Multimodal Language Models  \n",
       "   **Authors**: Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang  \n",
       "   **Summary**: This paper presents a baseline for applying multimodal language models (MLMs) to aerial detection in remote sensing, achieving impressive detection performance comparable to conventional models.  \n",
       "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09720v1) | [arXiv Link](http://arxiv.org/abs/2501.09720v1)\n",
       "\n",
       "3. **Title**: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps  \n",
       "   **Authors**: Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie  \n",
       "   **Summary**: This work explores the inference-time scaling behavior of diffusion models, revealing how performance can improve with increased computation during inference.  \n",
       "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09732v1) | [arXiv Link](http://arxiv.org/abs/2501.09732v1)\n",
       "\n",
       "4. **Title**: Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key  \n",
       "   **Authors**: Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li  \n",
       "   **Summary**: This paper addresses hallucination issues in large vision-language models using Direct Preference Optimization (DPO) and proposes an On-Policy Alignment framework to improve response accuracy.  \n",
       "   **Link**: [Download PDF](http://arxiv.org/pdf/2501.09695v1) | [arXiv Link](http://arxiv.org/abs/2501.09695v1)\n",
       "\n",
       "These papers provide insights into various aspects of multi-modal models and their applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to better visualize the answer,we use the display function\n",
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 0ae0bece-75ef-48f7-bc51-a3076a19eaf0. Step input:  download the following papers:\n",
      "for each paper:\n",
      "1. Process one paper at a time\n",
      "2. state which paper number you are processing of the total\n",
      "3.complete a full download cycle before moving to the next one\n",
      "4.explicitly state when moving to the next one\n",
      "5.provide a final summary only after all papers are downloaded\n",
      "\u001b[1;3;38;5;200mThought: I will start downloading the papers one at a time, as requested. I will begin with the first paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2501.09757v1', 'output_file_name': 'Distilling_Multi-modal_Large_Language_Models_for_Autonomous_Driving.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF file downloaded successfully to papers\\Distilling_Multi-modal_Large_Language_Models_for_Autonomous_Driving.pdf\n",
      "\u001b[0m> Running step 6f37eb42-65a5-4237-a4df-0211ceed86ec. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have completed the download of the first paper. Now I will proceed to download the second paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2501.09720v1', 'output_file_name': 'A_Simple_Aerial_Detection_Baseline_of_Multimodal_Language_Models.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF file downloaded successfully to papers\\A_Simple_Aerial_Detection_Baseline_of_Multimodal_Language_Models.pdf\n",
      "\u001b[0m> Running step 326f223a-b1d1-44a9-91c3-817530926793. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have completed the download of the second paper. Now I will proceed to download the third paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2501.09732v1', 'output_file_name': 'Inference_Time_Scaling_for_Diffusion_Models_beyond_Scaling_Denoising_Steps.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF file downloaded successfully to papers\\Inference_Time_Scaling_for_Diffusion_Models_beyond_Scaling_Denoising_Steps.pdf\n",
      "\u001b[0m> Running step e7a41ab1-a697-468f-8676-c287c49b21eb. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have completed the download of the third paper. Now I will proceed to download the fourth paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2501.09695v1', 'output_file_name': 'Mitigating_Hallucinations_in_Large_Vision_Language_Models_via_DPO.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF file downloaded successfully to papers\\Mitigating_Hallucinations_in_Large_Vision_Language_Models_via_DPO.pdf\n",
      "\u001b[0m> Running step ac4e3efb-59e7-4274-b09c-4cd6d65789a5. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have completed the download of all four papers. Now I will provide a final summary of the downloaded papers.\n",
      "Answer: 1. **Distilling Multi-modal Large Language Models for Autonomous Driving**  \n",
      "   - Authors: Deepti Hegde et al.  \n",
      "   - Summary: Proposes DiMA, an end-to-end autonomous driving system that distills information from a multi-modal large language model (LLM) to a vision-based planner, improving efficiency and reducing trajectory errors.  \n",
      "\n",
      "2. **A Simple Aerial Detection Baseline of Multimodal Language Models**  \n",
      "   - Authors: Qingyun Li et al.  \n",
      "   - Summary: Presents a baseline for applying multimodal language models (MLMs) to aerial detection in remote sensing, achieving impressive detection performance comparable to conventional models.  \n",
      "\n",
      "3. **Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps**  \n",
      "   - Authors: Nanye Ma et al.  \n",
      "   - Summary: Explores the inference-time scaling behavior of diffusion models, revealing how performance can improve with increased computation during inference.  \n",
      "\n",
      "4. **Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key**  \n",
      "   - Authors: Zhihe Yang et al.  \n",
      "   - Summary: Addresses hallucination issues in large vision-language models using Direct Preference Optimization (DPO) and proposes an On-Policy Alignment framework to improve response accuracy.  \n",
      "\n",
      "All papers have been successfully downloaded and are available for review.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#since the agent retains memory,we can use the same query template to ask for the agnet to download the papers.\n",
    "answer = agent.chat(''' download the following papers:\n",
    "for each paper:\n",
    "1. Process one paper at a time\n",
    "2. state which paper number you are processing of the total\n",
    "3.complete a full download cycle before moving to the next one\n",
    "4.explicitly state when moving to the next one\n",
    "5.provide a final summary only after all papers are downloaded''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Distilling Multi-modal Large Language Models for Autonomous Driving**  \n",
       "   - Authors: Deepti Hegde et al.  \n",
       "   - Summary: Proposes DiMA, an end-to-end autonomous driving system that distills information from a multi-modal large language model (LLM) to a vision-based planner, improving efficiency and reducing trajectory errors.  \n",
       "\n",
       "2. **A Simple Aerial Detection Baseline of Multimodal Language Models**  \n",
       "   - Authors: Qingyun Li et al.  \n",
       "   - Summary: Presents a baseline for applying multimodal language models (MLMs) to aerial detection in remote sensing, achieving impressive detection performance comparable to conventional models.  \n",
       "\n",
       "3. **Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps**  \n",
       "   - Authors: Nanye Ma et al.  \n",
       "   - Summary: Explores the inference-time scaling behavior of diffusion models, revealing how performance can improve with increased computation during inference.  \n",
       "\n",
       "4. **Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key**  \n",
       "   - Authors: Zhihe Yang et al.  \n",
       "   - Summary: Addresses hallucination issues in large vision-language models using Direct Preference Optimization (DPO) and proposes an On-Policy Alignment framework to improve response accuracy.  \n",
       "\n",
       "All papers have been successfully downloaded and are available for review."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want toask abot a topic which is not in the database\n",
    "#answer = agent.chat(query_template.format(topic= \"Quantum Computing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer = agent.chat(query_template.format(topic= \"Algebraic Topology\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markdown(answer.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
